{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f57bf77",
   "metadata": {},
   "source": [
    "## 일반적인 NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ebc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"nlp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456cf22",
   "metadata": {},
   "source": [
    "### Tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7484289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b001e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"NLP\",header=False,inferSchema=True,sep=\"\\t\")  # 일단 SMSSpamCollection로 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3731f9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if thats th...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4574aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"_c0\",\"label\").withColumnRenamed(\"_c1\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63a9d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\",outputCol=\"words\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\",outputCol=\"words\",pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4106a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                                                                                   |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[go, until, jurong, point,, crazy.., available, only, in, bugis, n, great, world, la, e, buffet..., cine, there, got, amore, wat...]                                                    |\n",
      "|[ok, lar..., joking, wif, u, oni...]                                                                                                                                                    |\n",
      "|[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005., text, fa, to, 87121, to, receive, entry, question(std, txt, rate)t&c's, apply, 08452810075over18's]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.transform(df)\n",
    "tokenized.select(\"words\").show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49f6f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                                                                                       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                                                 |\n",
      "|[ok, lar, joking, wif, u, oni]                                                                                                                                                              |\n",
      "|[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, question, std, txt, rate, t, c, s, apply, 08452810075over18, s]|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized = regexTokenizer.transform(df)\n",
    "regexTokenized.select(\"words\").show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37519ce2",
   "metadata": {},
   "source": [
    "## Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cae18c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5de8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = regexTokenized\n",
    "# text_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc0c9aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\",outputCol=\"cleaned\")\n",
    "text_df = remover.transform(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52a76ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|                text|               words|             cleaned|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|\n",
      "|  ham|Ok lar... Joking ...|[ok, lar, joking,...|[ok, lar, joking,...|\n",
      "| spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|\n",
      "|  ham|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|\n",
      "|  ham|Nah I don't think...|[nah, i, don, t, ...|[nah, think, goes...|\n",
      "| spam|FreeMsg Hey there...|[freemsg, hey, th...|[freemsg, hey, da...|\n",
      "|  ham|Even my brother i...|[even, my, brothe...|[even, brother, l...|\n",
      "|  ham|As per your reque...|[as, per, your, r...|[per, request, me...|\n",
      "| spam|WINNER!! As a val...|[winner, as, a, v...|[winner, valued, ...|\n",
      "| spam|Had your mobile 1...|[had, your, mobil...|[mobile, 11, mont...|\n",
      "|  ham|I'm gonna be home...|[i, m, gonna, be,...|[m, gonna, home, ...|\n",
      "| spam|SIX chances to wi...|[six, chances, to...|[six, chances, wi...|\n",
      "| spam|URGENT! You have ...|[urgent, you, hav...|[urgent, won, 1, ...|\n",
      "|  ham|I've been searchi...|[i, ve, been, sea...|[ve, searching, r...|\n",
      "|  ham|I HAVE A DATE ON ...|[i, have, a, date...|      [date, sunday]|\n",
      "| spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|\n",
      "|  ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|[oh, k, m, watching]|\n",
      "|  ham|Eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|\n",
      "|  ham|Fine if thats th...|[fine, if, that, ...|[fine, way, u, fe...|\n",
      "| spam|England v Macedon...|[england, v, mace...|[england, v, mace...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb8453",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6c0c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "## n-grams\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e8688f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|bigrams                                                                                                                                                                                                                                                                                                                                        |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[go until, until jurong, jurong point, point crazy, crazy available, available only, only in, in bugis, bugis n, n great, great world, world la, la e, e buffet, buffet cine, cine there, there got, got amore, amore wat]                                                                                                                     |\n",
      "|[ok lar, lar joking, joking wif, wif u, u oni]                                                                                                                                                                                                                                                                                                 |\n",
      "|[free entry, entry in, in 2, 2 a, a wkly, wkly comp, comp to, to win, win fa, fa cup, cup final, final tkts, tkts 21st, 21st may, may 2005, 2005 text, text fa, fa to, to 87121, 87121 to, to receive, receive entry, entry question, question std, std txt, txt rate, rate t, t c, c s, s apply, apply 08452810075over18, 08452810075over18 s]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram = NGram(n=2, inputCol=\"words\",outputCol=\"bigrams\")\n",
    "bigram_df = bigram.transform(text_df)\n",
    "bigram_df.select(\"bigrams\").show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323716df",
   "metadata": {},
   "source": [
    "## BOW 인코딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34abf817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF,  Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19f99f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                text|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|[go, until, juron...|\n",
      "|  ham|Ok lar... Joking ...|[ok, lar, joking,...|\n",
      "| spam|Free entry in 2 a...|[free, entry, in,...|\n",
      "|  ham|U dun say so earl...|[u, dun, say, so,...|\n",
      "|  ham|Nah I don't think...|[nah, i, don, t, ...|\n",
      "| spam|FreeMsg Hey there...|[freemsg, hey, th...|\n",
      "|  ham|Even my brother i...|[even, my, brothe...|\n",
      "|  ham|As per your reque...|[as, per, your, r...|\n",
      "| spam|WINNER!! As a val...|[winner, as, a, v...|\n",
      "| spam|Had your mobile 1...|[had, your, mobil...|\n",
      "|  ham|I'm gonna be home...|[i, m, gonna, be,...|\n",
      "| spam|SIX chances to wi...|[six, chances, to...|\n",
      "| spam|URGENT! You have ...|[urgent, you, hav...|\n",
      "|  ham|I've been searchi...|[i, ve, been, sea...|\n",
      "|  ham|I HAVE A DATE ON ...|[i, have, a, date...|\n",
      "| spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|\n",
      "|  ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|\n",
      "|  ham|Eh u remember how...|[eh, u, remember,...|\n",
      "|  ham|Fine if thats th...|[fine, if, that, ...|\n",
      "| spam|England v Macedon...|[england, v, mace...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"text\",outputCol=\"words\",pattern=\"\\\\W\")\n",
    "words_df = tokenizer.transform(df)\n",
    "words_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a21f5acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|                text|               words|         hashfeature|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|[go, until, juron...|(20,[0,3,4,7,8,10...|\n",
      "|  ham|Ok lar... Joking ...|[ok, lar, joking,...|(20,[3,5,9,10,15]...|\n",
      "| spam|Free entry in 2 a...|[free, entry, in,...|(20,[0,1,2,3,4,7,...|\n",
      "|  ham|U dun say so earl...|[u, dun, say, so,...|(20,[1,2,3,8,12,1...|\n",
      "|  ham|Nah I don't think...|[nah, i, don, t, ...|(20,[0,1,4,5,6,8,...|\n",
      "| spam|FreeMsg Hey there...|[freemsg, hey, th...|(20,[0,2,3,4,5,6,...|\n",
      "|  ham|Even my brother i...|[even, my, brothe...|(20,[3,5,6,8,9,10...|\n",
      "|  ham|As per your reque...|[as, per, your, r...|(20,[2,3,4,6,8,9,...|\n",
      "| spam|WINNER!! As a val...|[winner, as, a, v...|(20,[0,2,3,6,7,8,...|\n",
      "| spam|Had your mobile 1...|[had, your, mobil...|(20,[1,2,3,4,6,8,...|\n",
      "|  ham|I'm gonna be home...|[i, m, gonna, be,...|(20,[1,2,4,5,6,8,...|\n",
      "| spam|SIX chances to wi...|[six, chances, to...|(20,[0,1,3,4,6,7,...|\n",
      "| spam|URGENT! You have ...|[urgent, you, hav...|(20,[1,2,3,4,5,7,...|\n",
      "|  ham|I've been searchi...|[i, ve, been, sea...|(20,[0,1,2,3,4,5,...|\n",
      "|  ham|I HAVE A DATE ON ...|[i, have, a, date...|(20,[0,7,10,12,16...|\n",
      "| spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|(20,[0,1,2,3,5,6,...|\n",
      "|  ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|(20,[1,4,10,15,16...|\n",
      "|  ham|Eh u remember how...|[eh, u, remember,...|(20,[0,1,3,4,5,6,...|\n",
      "|  ham|Fine if thats th...|[fine, if, that, ...|(20,[0,3,4,5,7,9,...|\n",
      "| spam|England v Macedon...|[england, v, mace...|(20,[1,6,7,8,10,1...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\",outputCol=\"hashfeature\",numFeatures=20)\n",
    "hashed_df = hashingTF.transform(words_df)\n",
    "hashed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "135e9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"hashfeature\",outputCol=\"features\")\n",
    "idf_model = idf.fit(hashed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3be5e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  ham|(20,[0,3,4,7,8,10...|\n",
      "|  ham|(20,[3,5,9,10,15]...|\n",
      "| spam|(20,[0,1,2,3,4,7,...|\n",
      "|  ham|(20,[1,2,3,8,12,1...|\n",
      "|  ham|(20,[0,1,4,5,6,8,...|\n",
      "| spam|(20,[0,2,3,4,5,6,...|\n",
      "|  ham|(20,[3,5,6,8,9,10...|\n",
      "|  ham|(20,[2,3,4,6,8,9,...|\n",
      "| spam|(20,[0,2,3,6,7,8,...|\n",
      "| spam|(20,[1,2,3,4,6,8,...|\n",
      "|  ham|(20,[1,2,4,5,6,8,...|\n",
      "| spam|(20,[0,1,3,4,6,7,...|\n",
      "| spam|(20,[1,2,3,4,5,7,...|\n",
      "|  ham|(20,[0,1,2,3,4,5,...|\n",
      "|  ham|(20,[0,7,10,12,16...|\n",
      "| spam|(20,[0,1,2,3,5,6,...|\n",
      "|  ham|(20,[1,4,10,15,16...|\n",
      "|  ham|(20,[0,1,3,4,5,6,...|\n",
      "|  ham|(20,[0,3,4,5,7,9,...|\n",
      "| spam|(20,[1,6,7,8,10,1...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescale = idf_model.transform(hashed_df)\n",
    "rescale.select(\"label\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40e4c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0535f538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|                text|               words|            features|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|[go, until, juron...|           (5,[],[])|\n",
      "|  ham|Ok lar... Joking ...|[ok, lar, joking,...|           (5,[],[])|\n",
      "| spam|Free entry in 2 a...|[free, entry, in,...| (5,[1,3],[3.0,1.0])|\n",
      "|  ham|U dun say so earl...|[u, dun, say, so,...|           (5,[],[])|\n",
      "|  ham|Nah I don't think...|[nah, i, don, t, ...| (5,[0,1],[1.0,1.0])|\n",
      "| spam|FreeMsg Hey there...|[freemsg, hey, th...|(5,[0,1,2],[1.0,2...|\n",
      "|  ham|Even my brother i...|[even, my, brothe...|       (5,[1],[1.0])|\n",
      "|  ham|As per your reque...|[as, per, your, r...|       (5,[1],[1.0])|\n",
      "| spam|WINNER!! As a val...|[winner, as, a, v...|(5,[1,2,3],[2.0,1...|\n",
      "| spam|Had your mobile 1...|[had, your, mobil...| (5,[1,4],[2.0,2.0])|\n",
      "|  ham|I'm gonna be home...|[i, m, gonna, be,...| (5,[0,1],[3.0,1.0])|\n",
      "| spam|SIX chances to wi...|[six, chances, to...|       (5,[1],[3.0])|\n",
      "| spam|URGENT! You have ...|[urgent, you, hav...|(5,[1,2,3,4],[1.0...|\n",
      "|  ham|I've been searchi...|[i, ve, been, sea...|(5,[0,1,2,3,4],[3...|\n",
      "|  ham|I HAVE A DATE ON ...|[i, have, a, date...| (5,[0,3],[1.0,1.0])|\n",
      "| spam|XXXMobileMovieClu...|[xxxmobilemoviecl...| (5,[1,4],[1.0,2.0])|\n",
      "|  ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|       (5,[0],[1.0])|\n",
      "|  ham|Eh u remember how...|[eh, u, remember,...|       (5,[0],[2.0])|\n",
      "|  ham|Fine if thats th...|[fine, if, that, ...|       (5,[4],[2.0])|\n",
      "| spam|England v Macedon...|[england, v, mace...| (5,[1,4],[2.0,1.0])|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\",outputCol=\"features\",vocabSize=5, minDF=2.0)\n",
    "model = cv.fit(words_df)\n",
    "res = model.transform(words_df)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477649af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
