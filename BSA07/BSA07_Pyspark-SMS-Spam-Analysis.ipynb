{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea2b498",
   "metadata": {},
   "source": [
    "# SMS Spam Collection Dataset\n",
    "\n",
    "### 1. DESCRIPTION\n",
    "--------------\n",
    "\n",
    "The SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. \n",
    "\n",
    "#### 1.1. Compilation\n",
    "----------------\n",
    "\n",
    "This corpus has been collected from free or free for research sources at the Web:\n",
    "\n",
    "- A collection of between 425 SMS spam messages extracted manually from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http://www.grumbletext.co.uk/\n",
    "- A list of 450 SMS ham messages collected from Caroline Tag's PhD Theses available at http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf\n",
    "- A subset of 3,375 SMS ham messages of the NUS SMS Corpus (NSC), which is a corpus of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/\n",
    "- The amount of 1,002 SMS ham messages and 322 spam messages extracted from the SMS Spam Corpus v.0.1 Big created by Jos?Mar? G?ez Hidalgo and public available at: http://www.esp.uem.es/jmgomez/smsspamcorpus/\n",
    "\n",
    "\n",
    "#### 1.2. Statistics\n",
    "---------------\n",
    "\n",
    "There is one collection:\n",
    "\n",
    "- The SMS Spam Collection v.1 (text file: smsspamcollection) has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages.\n",
    "\n",
    "\n",
    "#### 1.3. Format\n",
    "-----------\n",
    "\n",
    "The files contain one message per line. Each line is composed by two columns: one with label (ham or spam) and other with the raw text. Here are some examples:\n",
    "\n",
    "ham   What you doing?how are you?\n",
    "ham   Ok lar... Joking wif u oni...\n",
    "ham   dun say so early hor... U c already then say...\n",
    "ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\n",
    "ham   Siva is in hostel aha:-.\n",
    "ham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\n",
    "spam   FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\n",
    "spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B\n",
    "spam   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\n",
    "\n",
    "Note: messages are not chronologically sorted.\n",
    "\n",
    "\n",
    "### 2. USAGE\n",
    "--------\n",
    "\n",
    "We offer a comprehensive study of this corpus in the following paper that is under review. This work presents a number of statistics, studies and baseline results for several machine learning methods.\n",
    "\n",
    "[1] Almeida, T.A., G?ez Hidalgo, J.M., Yamakami, A. Contributions to the study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (ACM DOCENG'11), Mountain View, CA, USA, 2011. (Under review)\n",
    "\n",
    "\n",
    "### 3. ABOUT\n",
    "--------\n",
    "\n",
    "The corpus has been collected by Tiago Agostinho de Almeida (http://www.dt.fee.unicamp.br/~tiago) and Jos?Mar? G?ez Hidalgo (http://www.esp.uem.es/jmgomez).\n",
    "\n",
    "We would like to thank Dr. Min-Yen Kan (http://www.comp.nus.edu.sg/~kanmy/) and his team for making the NUS SMS Corpus available. See: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/. He is currently collecting a bigger SMS corpus at: http://wing.comp.nus.edu.sg:8080/SMSCorpus/\n",
    "\n",
    "### 4. LICENSE/DISCLAIMER\n",
    "---------------------\n",
    "\n",
    "We would appreciate if:\n",
    "\n",
    "- In case you find this corpus useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\n",
    "- Send us a message to tiago@dt.fee.unicamp.br in case you make use of the corpus.\n",
    "\n",
    "The SMS Spam Collection v.1 is provided for free and with no limitations excepting:\n",
    "\n",
    "1. Tiago Agostinho de Almeida and Jos?Mar? G?ez Hidalgo hold the copyrigth (c) for the SMS Spam Collection v.1.\n",
    "\n",
    "2. No Warranty/Use At Your Risk. \n",
    "\n",
    "3. Limitation of Liability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57bf77",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ebc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456cf22",
   "metadata": {},
   "source": [
    "### Tokenzier & 토컨 개수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7484289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "# col: Returns a Column based on the given column name\n",
    "# udf: user defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7358381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "countTokenizer = udf(lambda w: len(w), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6175cb1",
   "metadata": {},
   "source": [
    "``` Python\n",
    "@udf(IntegerType())\n",
    "countTokenizer(w):\n",
    "    return len(w)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37519ce2",
   "metadata": {},
   "source": [
    "### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cae18c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323716df",
   "metadata": {},
   "source": [
    "### Term Freq-Inverse doc freq TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34abf817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285cbad",
   "metadata": {},
   "source": [
    "## 데이터 읽기 및 변수명(columns) 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99323d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"nlp_nb\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc26b851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if thats th...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"SMSSpamCollection\",inferSchema=True,sep=\"\\t\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1609fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            messages|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed(\"_c0\",\"label\").withColumnRenamed(\"_c1\",\"messages\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "787302a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08f802b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|label|            messages|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thats th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"length\",length(df[\"messages\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1a52e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"label\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df62ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature transformation\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c81184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"messages\",outputCol=\"tokened\")\n",
    "stop_word_remover = StopWordsRemover(inputCol=\"tokened\",outputCol=\"stoped\")\n",
    "count_vec = CountVectorizer(inputCol=\"stoped\",outputCol=\"c_vec\")\n",
    "idf = IDF(inputCol=\"c_vec\",outputCol=\"tf_idf\")\n",
    "ham_spam_to_num = StringIndexer(inputCol=\"label\",outputCol=\"label_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94378f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "cleaned = VectorAssembler(inputCols=['tf_idf','length'],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee21e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b58fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "파이프라인 = Pipeline(stages=[ham_spam_to_num, tokenizer,stop_word_remover,count_vec,idf,cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8eeb2fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            messages|length|label_01|             tokened|              stoped|               c_vec|              tf_idf|            features|\n",
      "+-----+--------------------+------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|   111|     0.0|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|(13423,[7,11,31,6...|(13424,[7,11,31,6...|\n",
      "|  ham|Ok lar... Joking ...|    29|     0.0|[ok, lar..., joki...|[ok, lar..., joki...|(13423,[0,24,297,...|(13423,[0,24,297,...|(13424,[0,24,297,...|\n",
      "| spam|Free entry in 2 a...|   155|     1.0|[free, entry, in,...|[free, entry, 2, ...|(13423,[2,13,19,3...|(13423,[2,13,19,3...|(13424,[2,13,19,3...|\n",
      "|  ham|U dun say so earl...|    49|     0.0|[u, dun, say, so,...|[u, dun, say, ear...|(13423,[0,70,80,1...|(13423,[0,70,80,1...|(13424,[0,70,80,1...|\n",
      "|  ham|Nah I don't think...|    61|     0.0|[nah, i, don't, t...|[nah, think, goes...|(13423,[36,134,31...|(13423,[36,134,31...|(13424,[36,134,31...|\n",
      "| spam|FreeMsg Hey there...|   147|     1.0|[freemsg, hey, th...|[freemsg, hey, da...|(13423,[10,60,139...|(13423,[10,60,139...|(13424,[10,60,139...|\n",
      "|  ham|Even my brother i...|    77|     0.0|[even, my, brothe...|[even, brother, l...|(13423,[10,53,103...|(13423,[10,53,103...|(13424,[10,53,103...|\n",
      "|  ham|As per your reque...|   160|     0.0|[as, per, your, r...|[per, request, 'm...|(13423,[125,184,4...|(13423,[125,184,4...|(13424,[125,184,4...|\n",
      "| spam|WINNER!! As a val...|   157|     1.0|[winner!!, as, a,...|[winner!!, valued...|(13423,[1,47,118,...|(13423,[1,47,118,...|(13424,[1,47,118,...|\n",
      "| spam|Had your mobile 1...|   154|     1.0|[had, your, mobil...|[mobile, 11, mont...|(13423,[0,1,13,27...|(13423,[0,1,13,27...|(13424,[0,1,13,27...|\n",
      "|  ham|I'm gonna be home...|   109|     0.0|[i'm, gonna, be, ...|[gonna, home, soo...|(13423,[18,43,120...|(13423,[18,43,120...|(13424,[18,43,120...|\n",
      "| spam|SIX chances to wi...|   136|     1.0|[six, chances, to...|[six, chances, wi...|(13423,[8,17,37,8...|(13423,[8,17,37,8...|(13424,[8,17,37,8...|\n",
      "| spam|URGENT! You have ...|   155|     1.0|[urgent!, you, ha...|[urgent!, won, 1,...|(13423,[13,30,47,...|(13423,[13,30,47,...|(13424,[13,30,47,...|\n",
      "|  ham|I've been searchi...|   196|     0.0|[i've, been, sear...|[searching, right...|(13423,[39,96,217...|(13423,[39,96,217...|(13424,[39,96,217...|\n",
      "|  ham|I HAVE A DATE ON ...|    35|     0.0|[i, have, a, date...|[date, sunday, wi...|(13423,[552,1697,...|(13423,[552,1697,...|(13424,[552,1697,...|\n",
      "| spam|XXXMobileMovieClu...|   149|     1.0|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(13423,[30,109,11...|(13423,[30,109,11...|(13424,[30,109,11...|\n",
      "|  ham|Oh k...i'm watchi...|    26|     0.0|[oh, k...i'm, wat...|[oh, k...i'm, wat...|(13423,[82,214,47...|(13423,[82,214,47...|(13424,[82,214,47...|\n",
      "|  ham|Eh u remember how...|    81|     0.0|[eh, u, remember,...|[eh, u, remember,...|(13423,[0,2,49,13...|(13423,[0,2,49,13...|(13424,[0,2,49,13...|\n",
      "|  ham|Fine if thats th...|    56|     0.0|[fine, if, thats...|[fine, thats, wa...|(13423,[0,74,105,...|(13423,[0,74,105,...|(13424,[0,74,105,...|\n",
      "| spam|England v Macedon...|   155|     1.0|[england, v, mace...|[england, v, mace...|(13423,[4,30,33,5...|(13423,[4,30,33,5...|(13424,[4,30,33,5...|\n",
      "+-----+--------------------+------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "전처리기 = 파이프라인.fit(df)\n",
    "전처리_df = 전처리기.transform(df)\n",
    "전처리_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f227bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train model and evaluation\n",
    "최종_df = 전처리_df.select(['label_01','features']).withColumnRenamed(\"label_01\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fddf2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df, test_df) = 최종_df.randomSplit([.75,.25], seed= 316)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07105c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[0,1,2,13,...|\n",
      "|  0.0|(13424,[0,1,2,41,...|\n",
      "|  0.0|(13424,[0,1,3,9,1...|\n",
      "|  0.0|(13424,[0,1,4,50,...|\n",
      "|  0.0|(13424,[0,1,5,20,...|\n",
      "|  0.0|(13424,[0,1,7,8,1...|\n",
      "|  0.0|(13424,[0,1,7,8,1...|\n",
      "|  0.0|(13424,[0,1,7,15,...|\n",
      "|  0.0|(13424,[0,1,9,14,...|\n",
      "|  0.0|(13424,[0,1,9,14,...|\n",
      "|  0.0|(13424,[0,1,11,32...|\n",
      "|  0.0|(13424,[0,1,12,33...|\n",
      "|  0.0|(13424,[0,1,14,18...|\n",
      "|  0.0|(13424,[0,1,14,31...|\n",
      "|  0.0|(13424,[0,1,14,78...|\n",
      "|  0.0|(13424,[0,1,15,20...|\n",
      "|  0.0|(13424,[0,1,17,19...|\n",
      "|  0.0|(13424,[0,1,18,20...|\n",
      "|  0.0|(13424,[0,1,30,12...|\n",
      "|  0.0|(13424,[0,1,43,69...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Naive Bayes Model\n",
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "nb = NaiveBayes()\n",
    "## labelCol:str=\"label\", featuresCol:str = \"features\", predictionCol:str=\"prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47e1928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "적합모형 = nb.fit(train_df)\n",
    "적합결과 = 적합모형.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fc6dee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:91.94272940617914\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "eval = MulticlassClassificationEvaluator()\n",
    "acc = eval.evaluate(적합결과)\n",
    "print(f\"Accuracy:{acc*100}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9a7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
