{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1e8429",
   "metadata": {},
   "source": [
    "# SMS Spam Collection Dataset\n",
    "\n",
    "### 1. DESCRIPTION\n",
    "--------------\n",
    "\n",
    "The SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. \n",
    "\n",
    "#### 1.1. Compilation\n",
    "----------------\n",
    "\n",
    "This corpus has been collected from free or free for research sources at the Web:\n",
    "\n",
    "- A collection of between 425 SMS spam messages extracted manually from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http://www.grumbletext.co.uk/\n",
    "- A list of 450 SMS ham messages collected from Caroline Tag's PhD Theses available at http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf\n",
    "- A subset of 3,375 SMS ham messages of the NUS SMS Corpus (NSC), which is a corpus of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/\n",
    "- The amount of 1,002 SMS ham messages and 322 spam messages extracted from the SMS Spam Corpus v.0.1 Big created by Jos?Mar? G?ez Hidalgo and public available at: http://www.esp.uem.es/jmgomez/smsspamcorpus/\n",
    "\n",
    "\n",
    "#### 1.2. Statistics\n",
    "---------------\n",
    "\n",
    "There is one collection:\n",
    "\n",
    "- The SMS Spam Collection v.1 (text file: smsspamcollection) has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages.\n",
    "\n",
    "\n",
    "#### 1.3. Format\n",
    "-----------\n",
    "\n",
    "The files contain one message per line. Each line is composed by two columns: one with label (ham or spam) and other with the raw text. Here are some examples:\n",
    "\n",
    "ham   What you doing?how are you?\n",
    "ham   Ok lar... Joking wif u oni...\n",
    "ham   dun say so early hor... U c already then say...\n",
    "ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\n",
    "ham   Siva is in hostel aha:-.\n",
    "ham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\n",
    "spam   FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\n",
    "spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B\n",
    "spam   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\n",
    "\n",
    "Note: messages are not chronologically sorted.\n",
    "\n",
    "\n",
    "### 2. USAGE\n",
    "--------\n",
    "\n",
    "We offer a comprehensive study of this corpus in the following paper that is under review. This work presents a number of statistics, studies and baseline results for several machine learning methods.\n",
    "\n",
    "[1] Almeida, T.A., G?ez Hidalgo, J.M., Yamakami, A. Contributions to the study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (ACM DOCENG'11), Mountain View, CA, USA, 2011. (Under review)\n",
    "\n",
    "\n",
    "### 3. ABOUT\n",
    "--------\n",
    "\n",
    "The corpus has been collected by Tiago Agostinho de Almeida (http://www.dt.fee.unicamp.br/~tiago) and Jos?Mar? G?ez Hidalgo (http://www.esp.uem.es/jmgomez).\n",
    "\n",
    "We would like to thank Dr. Min-Yen Kan (http://www.comp.nus.edu.sg/~kanmy/) and his team for making the NUS SMS Corpus available. See: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/. He is currently collecting a bigger SMS corpus at: http://wing.comp.nus.edu.sg:8080/SMSCorpus/\n",
    "\n",
    "### 4. LICENSE/DISCLAIMER\n",
    "---------------------\n",
    "\n",
    "We would appreciate if:\n",
    "\n",
    "- In case you find this corpus useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\n",
    "- Send us a message to tiago@dt.fee.unicamp.br in case you make use of the corpus.\n",
    "\n",
    "The SMS Spam Collection v.1 is provided for free and with no limitations excepting:\n",
    "\n",
    "1. Tiago Agostinho de Almeida and Jos?Mar? G?ez Hidalgo hold the copyrigth (c) for the SMS Spam Collection v.1.\n",
    "\n",
    "2. No Warranty/Use At Your Risk. \n",
    "\n",
    "3. Limitation of Liability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4c119",
   "metadata": {},
   "source": [
    "## 패키지 및 모듈 설치\n",
    "- pandas, numpy, nltk, scikit-learn(sklearn)\n",
    "- nltk를 import 한 후에는 stopwords 다운로드(한번만)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bc693",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1696e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# re: Regular expression operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31bafc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fba816",
   "metadata": {},
   "source": [
    "### 데이터 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1ff4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table(\"SMSSpamCollection\",header=None,sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b16249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           messages\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변수명(Column) 변경\n",
    "df = df.rename(columns={0: 'label',1: 'messages'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679b5c9",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "결측값이 없다는 가정 하에 처리되는 프로그램이 있음 \n",
    "\n",
    "ex) 지도에 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a788680e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label       0\n",
       "messages    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측자료 확인\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32412d5f",
   "metadata": {},
   "source": [
    "### Stopwords(불용어) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e65d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def 불용어제거(텍스트):\n",
    "    # 소문자로 변환\n",
    "    텍스트 = 텍스트.lower()\n",
    "    # 특수문자 제거\n",
    "    텍스트 = re.sub(r'[^0-9a-zA-Z]', ' ', 텍스트)\n",
    "    # 추가 빈 칸 제거\n",
    "    텍스트 = re.sub(r'\\s+', ' ', 텍스트)\n",
    "    # remove stopwords\n",
    "    텍스트 = \" \".join(word for word in 텍스트.split() if word not in STOPWORDS)\n",
    "    return 텍스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871cea8",
   "metadata": {},
   "source": [
    "### re모듈\n",
    "- regex: 정규표현식\n",
    "- 특수문자 처리 용이\n",
    "- 함수: match, search, findall, finditer, fullmatch, split, sub, subn, compile, purge, escape 등\n",
    "- sub(패턴, 교체할 문자열, 문자열, 최대 교체 수, 플래그)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7068fc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messages</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           messages  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                           processed  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4             nah think goes usf lives around though  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# messages에 불용어제거 적용\n",
    "df['processed'] = df['messages'].apply(불용어제거)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "371804eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할(processed => X, label => y)\n",
    "X = df['processed']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc83d7d",
   "metadata": {},
   "source": [
    "## BOW(Bag of Words) 인코딩\n",
    "문서를 숫자 벡터로 변환하는 가장 기본적인 방법\n",
    "\n",
    "BOW 인코딩 방법에서는 전체 문서 $\\{d_1,d_2, \\cdots,d_n\\}$를 구성하는 고정된 단어장(vocabulary) $\\{t_1,t_2,\\cdots,t_m\\}$를 만들고 $d_i$라는 개별 문서에 단어장에 해당하는 단어들이 포함되어 있는지를 표시하는 방법\n",
    "\n",
    "$$x_{i,j} = \\mbox{문서 } d_i \\mbox{내의 단어 } t_j \\mbox{의 출현 빈도} $$\n",
    "\n",
    "\n",
    "Scikit-Learn 문서 전처리 기능\n",
    "- DictVectorizer: 각 단어의 수를 세어놓은 사전에서 BOW 인코딩 벡터 생성\n",
    "- CountVectorizer: 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩 벡터\n",
    "    - 문서를 토큰 리스트로 변환\n",
    "    - 각 문서에서 토큰의 출현 빈도를 계산(소$\\cdot$대문자 구분하지 않음)\n",
    "    - 각 문서를 BOW 인코딩 벡터로 변환\n",
    "- TfidfVectorizer: TF-IDF(Term Frequency-Inverse Document Frequency) 방식으로 단어의 가중치를 조정한 BOW 인코딩 벡터 생성\n",
    "\n",
    "$$ \\mbox{TF-IDF}(w) = \\mbox{TF}(w) \\times \\log \\left( \\frac{N}{\\mbox{DF}(w)} \\right) $$\n",
    "\n",
    "- HashingVectorizer: 해시 함수(hash function)을 사용하여 적은 메모리와 빠른 속도로 BOW 인코딩 벡터 생성\n",
    "- TfidfTransformer: 카운트 행렬을 표준화 된 tf 또는 tf-idf 표현으로 변환\n",
    "\n",
    "## Model Training\n",
    "\n",
    "- 데이터를 학습자료와 검증자료로 나눔\n",
    "- 학습자료를 이용하여 모델 학습\n",
    "    - CountVectorizer $\\longrightarrow$  TfidfTransformer $\\longrightarrow$ 설명변수로 모형 적용\n",
    "- 검증자료로 정확도 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08d7817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb7f419",
   "metadata": {},
   "source": [
    "일반적인 과정\n",
    "``` python\n",
    "def classify(model, X, y):\n",
    "    # train test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=316, shuffle=True, stratify=y)\n",
    "    # model training\n",
    "    model.fit(X_train, y_train)    \n",
    "    print('Accuracy:', model.score(x_test, y_test)*100)\n",
    "    cv_score = cross_val_score(model, X, y, cv=5)\n",
    "    print(\"CV Score:\", np.mean(cv_score)*100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3eb7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=316, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d33c94",
   "metadata": {},
   "source": [
    "### sklearn.feature_extraction.text.CountVectorizer\n",
    "Convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "503e62eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "예제문장 =  ['This is the first document.', 'This document is the second document.', \n",
    "         'And this is the third one.', 'Is this the first document?']\n",
    "vectorizer = CountVectorizer()\n",
    "토큰개수 = vectorizer.fit_transform(예제문장)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d62e93be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca7af765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(토큰개수.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77ceccd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
       "       'second document', 'the first', 'the second', 'the third',\n",
       "       'third one', 'this document', 'this is', 'this the'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "토큰개수2 =  vectorizer2.fit_transform(예제문장)\n",
    "vectorizer2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b3e1eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(토큰개수2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05e20cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['this', 'document', 'first', 'is', 'second', 'the', 'and', 'one'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "단어장 = ['this', 'document', 'first', 'is', 'second', 'the', 'and', 'one'] # 이 단어들의 빈도\n",
    "vectorizer3 = CountVectorizer(vocabulary=단어장)\n",
    "토큰개수3 =  vectorizer3.fit_transform(예제문장)\n",
    "vectorizer3.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abeca164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 0 1 0 0]\n",
      " [1 2 0 1 1 1 0 0]\n",
      " [1 0 0 1 0 1 1 1]\n",
      " [1 1 1 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(토큰개수3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dd1e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': 0, 'second': 1}\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = 'english').fit(예제문장) # stop_words 제외하고\n",
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de43186a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.        ]\n",
      " [0.78722298 0.61666846]\n",
      " [0.         0.        ]\n",
      " [1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.transform(예제문장).toarray()) # 가중치 고려 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd4bea",
   "metadata": {},
   "source": [
    "### sklearn.feature_extraction.text.TfidfTransformer\n",
    "Transform a count matrix to a normalized tf or tf-idf representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c099c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "파이프 = Pipeline([('count', CountVectorizer(vocabulary=단어장)),\n",
    "                 ('tfid', TfidfTransformer())]).fit(예제문장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc3b25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "토큰개수  = 파이프['count'].transform(예제문장) # 특정 작업을 하게 만들 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7eaf798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 0 1 0 0]\n",
      " [1 2 0 1 1 1 0 0]\n",
      " [1 0 0 1 0 1 1 1]\n",
      " [1 1 1 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(토큰개수.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "392e4288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
       "       1.        , 1.91629073, 1.91629073])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "파이프['tfid'].idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cf83b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38408524, 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.38408524, 0.        , 0.        ],\n",
       "       [0.28108867, 0.6876236 , 0.        , 0.28108867, 0.53864762,\n",
       "        0.28108867, 0.        , 0.        ],\n",
       "       [0.31091996, 0.        , 0.        , 0.31091996, 0.        ,\n",
       "        0.31091996, 0.59581303, 0.59581303],\n",
       "       [0.38408524, 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.38408524, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "파이프.transform(예제문장).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc659783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMS분류(model, X, y):\n",
    "    # train test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=316, shuffle=True, stratify=y)\n",
    "    # model training\n",
    "    pipeline_model = Pipeline([('vect', CountVectorizer()),\n",
    "                               ('tfidf',TfidfTransformer()),\n",
    "                               ('clf', model)])\n",
    "    pipeline_model.fit(x_train, y_train)    \n",
    "    print('Accuracy:', pipeline_model.score(x_test, y_test)*100)    \n",
    "    \n",
    "    y_pred = pipeline_model.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cv_score = cross_val_score(pipeline_model, X, y, cv=5)\n",
    "    print(\"CV Score:\", np.mean(cv_score)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd017772",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "혼동행렬(Confusion matrix)\n",
    "\n",
    "분류 \\ 실제 | Positive | Negative\n",
    "------|--------|--------\n",
    "Pasitive | TP  | FP\n",
    "Negative |  FN | TN\n",
    "\n",
    "- 정확도(accuracy) = (TP+TN)/(TP+FN+FP+TN)\n",
    "- 정밀도(precision) = TP/(TP+FP)\n",
    "- 재현율(recall, sensitivity) = TP/(TP+FN)\n",
    "- F1 = 2 (precision x recall)/(precision+recall)\n",
    "    - precision과 recall의 조화평균, imbalanced 구조인 경우 많이 사용\n",
    "- macro: label별 각 합의 평균\n",
    "- micro: 전체평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e36cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.55419956927494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1206\n",
      "        spam       0.98      0.76      0.86       187\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.97      0.88      0.92      1393\n",
      "weighted avg       0.97      0.97      0.96      1393\n",
      "\n",
      "CV Score: 96.48235663508065\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "SMS분류(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f60772bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.98492462311557\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98      1206\n",
      "        spam       1.00      0.78      0.87       187\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.98      0.89      0.93      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n",
      "CV Score: 96.93101255122333\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "SMS분류(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec6ad3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.42067480258436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       1.00      0.88      0.94       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.99      0.94      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "CV Score: 98.15137145663428\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(C=3)\n",
    "SMS분류(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "107bea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.4156496769562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99      1206\n",
      "        spam       0.99      0.81      0.89       187\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.98      0.91      0.94      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n",
      "CV Score: 97.50529341201664\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "SMS분류(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c328a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMS예측(model, x_train, y_train, x_test):\n",
    "    pipeline_model = Pipeline([('vect', CountVectorizer()),\n",
    "                               ('tfidf',TfidfTransformer()),\n",
    "                               ('clf', model)])\n",
    "    pipeline_model.fit(x_train, y_train)    \n",
    "    y_pred = pipeline_model.predict(x_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "454b5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=316, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "525499c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=3)\n",
    "예측label = SMS예측(model, x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "953eb784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test),type(예측label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a25754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham']\n",
      "['ham' 'ham' 'ham' 'ham' 'spam' 'ham' 'ham' 'ham' 'spam' 'ham']\n"
     ]
    }
   ],
   "source": [
    "print(list(y_test[:10]))\n",
    "print(예측label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f17f18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1206    0]\n",
      " [  22  165]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "혼동행렬 = confusion_matrix(y_test, 예측label)\n",
    "print(혼동행렬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58fd5447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9820846905537459\n",
      "Recall: 1.0\n",
      "F1: 0.9909613804437141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Precision:\",precision_score(y_test, 예측label, pos_label=\"ham\"))\n",
    "print(\"Recall:\",recall_score(y_test, 예측label, pos_label=\"ham\"))\n",
    "print(\"F1:\",f1_score(y_test, 예측label, pos_label=\"ham\"))      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b380222",
   "metadata": {},
   "source": [
    "### 개선방안 마련\n",
    "- TF-IDF 방식 이외의 방법으로 임베딩\n",
    "- 적용된 모형에서는 default 옵션 사용했는데 옵션 변경\n",
    "\n",
    "### 기계학습의 문제점\n",
    "- 딥러닝을 시행했을 때, 뭘 완벽하게 예측할 수 있다고?\n",
    "- 근데 그게 의미가 있나?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f33d7d",
   "metadata": {},
   "source": [
    "### 한글자료인 경우에 추가적으로 해야 하는 작업은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd90dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
